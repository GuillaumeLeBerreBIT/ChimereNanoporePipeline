Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 8
Rules claiming more threads will be scaled down.
Job stats:
job                 count    min threads    max threads
----------------  -------  -------------  -------------
ProwlerTrim             1              1              1
SACRAcall               1              1              1
StatisticsToHTML        1              1              1
all                     1              1              1
porechopABIcall         1              1              1
total                   5              1              1

Select jobs to execute...

[Wed May  3 08:57:26 2023]
rule porechopABIcall:
    input: PorechopABI/fastq_runid_211de24bb98b581ec357aee6dd1409fc7b321927_0_0.fastq
    output: PorechopABI/Output_reads.fastq, reports/Statistics.txt
    jobid: 2
    reason: Forced execution
    resources: tmpdir=/tmp

Activating conda environment: .snakemake/conda/8ff5eb033241f5889665bcedcbfab59f_
[Wed May  3 08:58:54 2023]
Finished job 2.
1 of 5 steps (20%) done
Select jobs to execute...

[Wed May  3 08:58:54 2023]
rule ProwlerTrim:
    input: PorechopABI/Output_reads.fastq
    output: ProwlerProcessed/Output_readsTrimLT-U0-D7W100L100R0.fasta
    jobid: 3
    reason: Input files updated by another job: PorechopABI/Output_reads.fastq
    resources: tmpdir=/tmp

[Wed May  3 08:58:55 2023]
Finished job 3.
2 of 5 steps (40%) done
Select jobs to execute...

[Wed May  3 08:58:55 2023]
rule SACRAcall:
    input: ProwlerProcessed/Output_readsTrimLT-U0-D7W100L100R0.fasta
    output: SACRAResults/sacra_results.fasta
    jobid: 4
    reason: Input files updated by another job: ProwlerProcessed/Output_readsTrimLT-U0-D7W100L100R0.fasta
    resources: tmpdir=/tmp

RuleException in rule SACRAcall in file /home/guillaume/ChimereNanoporePipeline/Snakefile, line 32:
AttributeError: 'InputFiles' object has no attribute 'in_prow', when formatting the following:
scripts/SACRA.sh -i {input.in_prow} -p {output} -t 6 -c config_sacra.yml
