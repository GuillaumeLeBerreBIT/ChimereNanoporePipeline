#!/usr/bin/python3
############################# INTRODUCTION #############################
# Author: Guillaume Le Berre
# GitHub: https://github.com/GuillaumeLeBerreBIT
# 
# Purpose: The script takes all different imput files generated using different Bio-informatic tools & results from scripts (photos, text files).
# The finalized output will be a HTML report file. Where each chapter has a summary of the output files on the amount of reads, the length of reads, 
# how many chimeras, assembly visualization & summary of the contig results. 
# 
# Porechop ABI
# Prowler Trimming
# SACRA
# SACRA Filtered
# DIAMOND
# Flye - BANDAGE
# MITOS2
# Contig X 
###################################################################
# 
############################# MODULES #############################
# Importing all necassary librarys to execute the script. Webbrowser & time can be implemented in the future. 

import re, os, argparse #, webbrowser, time
from Bio import SeqIO   # pip install biopython
import numpy as np      # pip install numpy
import matplotlib.pyplot as plt     # pip install matplotlib
import pandas as pd

############################# COMMAND LINE INPUT #############################

parser = argparse.ArgumentParser(description='Generate Statistical Report. The report will contain the results in graph, tables, text, figures fo all different input files/folders gathered throughout the Snakemake pipeline.')
parser.add_argument('outputFile', type=str, 
                    help='The name of the folder that will contain the output files.')                       
parser.add_argument('porechopStat', type=str, 
                    help='The folder containing the Porechop Statistic files. THe files are the output from the terminal printed in text files.')
parser.add_argument('porechopFastq', type=str, 
                    help='The folder containing the fastq files generated by Porechop ABI.')                                  
parser.add_argument('prowlerFolder', type=str, 
                    help='Give the folder containing the fasta files generated by Prowler Trimmer.')
parser.add_argument('sacraFiltered', type=str, 
                    help='Give the Fasta file containing the filtered sequences on a certain length.')
parser.add_argument('BandageFlye', type=str, 
                    help='Give the image name from the Flye assembly graph with BANDAGE.')
parser.add_argument('MITOS2Folder', type=str, 
                    help='Give the folder with the MITOS2 output folders.')
args = parser.parse_args()

## PARSING UNIQUE IDENTIFIER
# Use the unique identifier per sample 
uniquelabel = args.outputFile

# Will use the unique label, ued to create a folder as identifier. Split on paths ('/') and get the unique identifier name out of the list. 
splitted_label = uniquelabel.split("/")
identifier = splitted_label[2]

############################# HANDLING FILES #######################################
# This section will contain each of the files used for statistical summary;

############################# PORECHOP REPORT  #######################################
## TERMINAL OUTPUT
# Empty list == The list used to gather all the lines containing relevant information from the terminal 
statistics_list = []

# Go over all the files in the directory
for file in os.listdir(args.porechopStat):
    
    # Combining the path to the file + file name -- > To open the file
    file_path = f"{args.porechopStat}/{file}" 
    # Opening the file from the current location 
    with open(file_path, "r") as text_file:
        # Readlines returns a list of lines split on newline character == "\n"
        splitted_file = text_file.readlines()

        # Flag == Define when to print/parse or not. 
        flag = 0 

        for line in splitted_file:
            # Setting up the different conditions to handle the output
            # When the re.search matches the pattern of a line it will return == True if not == False
            if re.search("Trimming adapters from read ends", line):
                flag = 1
            elif re.search("adapters trimmed from their", line):
                flag = 2
            elif re.search("reads were split based on middle adapters", line):
                flag = 3
            # When encoutering a newline flag == 0 == Not add the line to the list
            elif re.search("^\n", line):
                flag = 0

            # If any of the flags match the set value == strip the line on unwanted characters & save the line in a list
            if flag == 1 or flag == 2 or flag == 3:
                
                # Removing leading or trailing characters
                stripped_line = line.strip()
                # Add each stripped line to the list 
                statistics_list.append(stripped_line)     
        

# Empty list == Removing special characters from the lines. 
cleaned_text_list = []

for text in statistics_list:
    # If any of the patterns match in the item from statistics_list then replace it by nothing == Removing unwanted characters. 
    cleaned_text = re.sub(r'\x1b\[1m\x1b\[4m|\x1b\[0m|\x1b\[31m', '', text)
    # If the filtered item has nothing == Delete/Skip
    # Else keep it and add to a list 
    if cleaned_text == '':
        continue
    else: 
        cleaned_text_list.append(cleaned_text)   

## ADAPTERS

# Define an empty set == Will keep unique items only in a list (no duplicates). 
adapters = set()
# Save all the statistical stats of trimmed reads in a list. 
trimmed_count = []
# Loop over the list that has been fully cleaned/stripped
for item in cleaned_text_list:
    # ADAPTER LINES == ":"
    if re.search(":", item):
        adapters.add(item)
    # REMOVED READS STATS == "/"
    elif re.search("/", item):
        trimmed_count.append(item)
# Need to extract now Start - Mid - End
start = []
middle = []
end = []

# Gather the lines containg start - middle - end in each respective list. 
for item in trimmed_count:

    if re.search("start", item):
        start.append(item)
    elif re.search("middle", item):
        middle.append(item)
    elif re.search("end", item):
        end.append(item)

StartAdapRem = 0 
StartTotReads = 0
StartBasePairs = 0

# Split each item == sentence -- > Get the numbers only to generate a summary of all files togheter. 
for s in start:
    splitted_start = s.split(" ")
    # [0] == Nr reads adapters removed & [2] == Total Reads & [10] == BP
    StartAdapRem += int(splitted_start[0].replace(",",""))
    StartTotReads += int(splitted_start[2].replace(",",""))
    StartBasePairs += int(splitted_start[10].replace(",","").replace("(",""))

#print(f"{StartAdapRem}-{StartTotReads}-{StartBasePairs}")

MidAdapRem = 0 
MidTotReads = 0

# Split each item == sentence -- > Get the numbers only to generate a summary of all files togheter.
for m in middle:
    splitted_mid = m.split(" ")
    # [0] == Nr reads adapters removed & [2] == Total Reads 
    MidAdapRem += int(splitted_mid[0].replace(",",""))
    MidTotReads += int(splitted_mid[2].replace(",",""))

#print(f"{MidAdapRem}-{MidTotReads}")

EndAdapRem = 0 
EndTotReads = 0
EndBasePairs = 0

# Split each item == sentence -- > Get the numbers only to generate a summary of all files togheter.
for e in end:
    splitted_end = e.split(" ")
    # [0] == Nr reads adapters removed & [2] == Total Reads & [10] == BP
    EndAdapRem += int(splitted_end[0].replace(",",""))
    EndTotReads += int(splitted_end[2].replace(",",""))
    EndBasePairs += int(splitted_end[10].replace(",","").replace("(",""))

#print(f"{EndAdapRem}-{EndTotReads}-{EndBasePairs}")

############################# PROWLER REPORT #############################
# Empty lists to store the read lengths before and after trimming
before_trim = []
after_trim = []

# Biopython library can handle fastq & fasta files. 
# Makes it much more easier to read in files and can filter on what part of each sequence you want == .seq, .id 
# Much faster then writing it yourself. 
# Read the input fastq file and append the length of each read to before_trim list. 
for pore_file in os.listdir(args.porechopFastq):
    # Concat the filepath == To open the file
    file_path_pore = f"{args.porechopFastq}/{pore_file}"

    for seq_record in SeqIO.parse(file_path_pore, "fastq"):
            # The length of each read
            before_trim.append(len(seq_record.seq))

# Read the trimmed fasta file and append the length of each read to after_trim list
for prow_file in os.listdir(args.prowlerFolder):   
    # Due to other files present (output SACRA), only want the fasta file before SACRA. 
    if not re.search(".csv",prow_file) or\
        re.search(".non_chimera.fasta",prow_file) or\
        re.search(".split.fasta", prow_file):
          
        file_path_prow = f"{args.prowlerFolder}/{prow_file}"

        for seq_record in SeqIO.parse(file_path_prow, "fasta"):
            after_trim.append(len(seq_record.seq))

# Convert the read length lists to numpy arrays >> Plots require numpy array == [[...]]
before_array = np.array(before_trim)
after_array = np.array(after_trim)

# Create a figure with two subplots + tight layout
fig, axs = plt.subplots(1, 2, tight_layout=True, figsize = (10,5))

# Plot a histogram of read lengths before trimming + setting number of bins + setting the range of x axis
axs[0].hist(before_array, bins = 40, range = [0,10000])
axs[0].set_title('Before trimming Reads')
axs[0].set_xlabel('Read length')
axs[0].set_ylabel('Frequency')

# Plot a histogram of read lengths before trimming + setting number of bins + setting the range of x axis
axs[1].hist(after_array, bins = 40, range = [0,10000])
axs[1].set_title('After trimming Reads')
axs[1].set_xlabel('Read length')
axs[1].set_ylabel('Frequency')

# Saving the graph picture. 
plt.savefig(f"../results/{identifier}/{identifier}Before&After-Prowler.png", dpi=200)
# Savefig does not close the plot. >> clf = close
plt.clf()

############################# SACRA REPORT #############################
# Defining empty lists beforehand == Amount of records
prow_records = []
chim_records = []
nonchim_records = []
# Gather seq len from Non-Cimera and Chimera reads -- > After SACRA 
sacra_seq_len = []

### PROWLER SEQUENCES
# Loop over the files in the folder. 
for prow_file in os.listdir(args.prowlerFolder):   
    # Want one specif file == .fasta
    if not re.search(".csv",prow_file) and\
        re.search(".non_chimera.fasta",prow_file) and\
        re.search(".split.fasta", prow_file):
          
        file_path_prow = f"{args.prowlerFolder}/{prow_file}"
        # Reading in the Prowler Fasta file 
        for seq_record in SeqIO.parse(file_path_prow, "fasta"):
            # Append each record to a list
            prow_records.append(seq_record.id)

# Counting the number of IDs == Number of sequences >> total number of sequences present in the fasta file.
count_prow = len([record for record in prow_records])

### CHIMERA SEQUENCES
# Loop over the files in the folder.
for chim_file in os.listdir(args.prowlerFolder):

    if re.search(".split.fasta", chim_file):
        
        file_path_chim = f"{args.prowlerFolder}/{chim_file}"
        # Reading in the Fasta file with Chimere sequences
        for seq_record in SeqIO.parse(file_path_chim, "fasta"):
            # Append each record/header to a list
            chim_records.append(seq_record.id)
            # Gathering the read lengths. >> Want all read lengths (Chimera sequences multiple headers same BUT different Start - End position added)
            sacra_seq_len.append(len(seq_record.seq))

# Counting the number of IDs == Number of sequences >> total number of sequences present in the fasta file.
count_chim = len([record for record in chim_records])

### UNIQUE CHIMERA SEQUENCE ID
# Will try to count and see how many reads had one/multiple chimera reads == UNIQUE HEADERS!! >> .split.fasta
# Setting empty list beforehand
record_splitted = []
# Iterating over the collected chimera sequence records
for chim in chim_records:
    # Split each item >> HEADER:START-END == Want only the first part to find unique headers
    chim_splitted = chim.split(":")
    # Only ID part retained
    record_splitted.append(chim_splitted[0])
# Set an empty set >> Set == only unique IDs
unique_chim = set()
# Iterate over the sequence IDs
for i in record_splitted:
    # Add to the set of IDs
    unique_chim.add(i)
# Counting the number of IDs == Number of UNIQUE sequences
count_unique_chim = len([record for record in unique_chim])

### NON CHIMERA SEQUENCES
for chim_file in os.listdir(args.prowlerFolder):

    if re.search(".non_chimera.fasta", chim_file):
        
        file_path_non_chim = f"{args.prowlerFolder}/{chim_file}"
        # Non chimera sequences were not splitted or so thus headers remain unique
        # Reading in the SACRA fasta file with Non Chimere reads
        for seq_record in SeqIO.parse(file_path_non_chim, "fasta"):
            nonchim_records.append(seq_record.id)
            # Gathering the read lengths. 
            sacra_seq_len.append(len(seq_record.seq))

# Counting the number of IDs == Number of reads
count_nonchim = len([record for record in nonchim_records])

# Informative print of the results gathered
#print(f"Reads after Prowler: {count_prow}\
#      \nHow many chimera sequences: {count_chim}\
#      \nHow many from original sequence, are unique: {count_unique_chim}\
#      \nHow many non chimera sequences: {count_nonchim}\
#      \nSum chimera unique IDs & non chimera IDs: {(total_sacra_seq := count_unique_chim + count_nonchim)}")

### VISUALIZATION
### RAW RESULTS
# Creating a pandas dataframe >> Parse to matplotlib
sacraDf = pd.DataFrame([["No. sequences", count_unique_chim, count_nonchim]],
                       columns = ["Amount", "Chimera", "Non chimera"])
# Plotting the rows  of the No. sequences per bar.
# Setting the width of the bars bit smaller. 
# Adding colormap for visualization. 
ax = sacraDf.plot(x='Amount', kind='bar', stacked=True, width = 0.2,
                colormap = "Set3",  
                title='Total amounft of chimera and non-chimera sequences after SACRA')

# Iterating over the patches to obtain the width and height + x and y coordinates
# Using the x, y coordinates to place the label in the center of corresponding bar
for p in ax.patches:
    width, height = p.get_width(), p.get_height()
    x, y = p.get_xy() 
    # labelling text based on gathered positions. 
    ax.text(x+width/2, 
            y+height/2, 
            '{:.0f}'.format(height), 
            horizontalalignment='center', 
            verticalalignment='center')
    
# For some reason have to set the ticks to 0 to get the label horizontally. 
plt.xticks(rotation=0)
# Legend location to the upper right
# bbox anchor is to change the location, placed it outside the box, bbox_to_anchor(x,y)
plt.legend(loc = 'upper right', bbox_to_anchor=(1.4, 0.95))
# Shrink current axis by 20% (x-axis)
box = ax.get_position()
ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])
# Label y-axis
plt.ylabel("No. of sequences")
# Saving the picture 
plt.savefig(f"../results/{identifier}/{identifier}SACRA-Stacked-Seq-Amount.png", dpi=200)
# Savefig does not close the plot. 
plt.clf()

### RELATIVE RESULTS
# Calculations Relative Amount
total_sacra_seq = count_unique_chim + count_nonchim
rel_unique_chim = (count_unique_chim / total_sacra_seq) * 100
rel_nonchim = (count_nonchim / total_sacra_seq) * 100
# Creating a pandas dataframe. >> Parse to matplotlib
sacraDf = pd.DataFrame([["No. sequences", rel_unique_chim, rel_nonchim]],
                       columns = ["Amount", "Chimera", "Non chimera"])
#Plotting the rows  of the No. sequences per bar.
# Setting the width of the bars bit smaller. 
# Adding colormap for visualization. 
ax = sacraDf.plot(x='Amount', kind='bar', stacked=True, width = 0.2,
                colormap = "Set3",  
                title='Total amounft of chimera and non-chimera sequences after SACRA')
# Iterating over the patches to obtain the width and height + x and y coordinates
# Using the x, y coordinates to place it in the center of corresponding bar
for p in ax.patches:
    width, height = p.get_width(), p.get_height()
    x, y = p.get_xy() 
    # labelling text based on gathered positions. 
    ax.text(x+width/2, 
            y+height/2, 
            '{:.2f} %'.format(height), 
            horizontalalignment='center', 
            verticalalignment='center')

# For some reason have to set the ticks to 0 to get the label horizontally. 
plt.xticks(rotation=0)
# Shrink current axis by 20%
box = ax.get_position()
ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])
# Legend location to the upper right
# bbox anchor is to change the location, placed it outside the box, bbox_to_anchor(x,y)
plt.legend(loc = 'upper right', bbox_to_anchor=(1.4, 0.95))
# Label y-axis
plt.ylabel("No. of sequences")
# Saving the created plot as .png, using the dpi to set the size of the figure. 
plt.savefig(f"../results/{identifier}/{identifier}SACRA-Stacked-Seq-Rel-Amount.png", dpi=200)
# Savefig does not close the plot. 
plt.clf()
### HISTOGRAM LENGTH READS
# Convert the read length lists to numpy arrays for plotting
sacra_array = np.array(sacra_seq_len)

# Plot a histogram of sequence lengths after SACRA
# Setting amount of bins & range of the graph. 
plt.hist(sacra_array, bins = 40, range = [min(sacra_array), 1000])
# Setting title, x and y labels. 
plt.title('Sequence lengths after SACRA')
plt.xlabel('Sequence length')
plt.ylabel('Frequency')
# Determining to show the interval of x-axis ticks. 
plt.xticks(np.arange(0, 1000, 100))
# Saving the figure in .png format. 
plt.savefig(f"../results/{identifier}/{identifier}SACRA-Hist-Distribution.png", dpi=200)
# Savefig does not close the plot. 
plt.clf()

#######################################
# SACRA FILTERING STEP
#######################################
# Lists to store the sequence lengths after the filtering of fasta file on certain length. 
filtered_sacra = []
# Read the input fastq file and append the length of each sequence to before_trim list
for seq_record in SeqIO.parse(args.sacraFiltered, "fasta"):
    filtered_sacra.append(len(seq_record.seq))
# Convert the read length lists to numpy arrays for plotting
before_array = np.array(filtered_sacra)

# Plot a histogram with a predefined number of bins & a range set from x1 to x2.
plt.hist(before_array, bins=40, range=[0,1000])
# Setting the title
plt.title('Filtering on a treshhold of ' + str(min(filtered_sacra)) + ' bases')
# X-axis label
plt.xlabel('Read length')
# Setting y-axis label
plt.ylabel('Frequency')
# Detrmining to show the interval of x-axis ticks. 
# np.arange to go from a to b in x amount of steps. 
plt.xticks(np.arange(0, 1000, 100))
# Saving the figure 
plt.savefig(f"../results/{identifier}/{identifier}SACRA-Hist-FilteredSeq.png", dpi=200)
# Close the plot
plt.clf()

#######################################
# GENERATING HTML REPORT
#######################################
# Creating the header line
html_header = f"<! DOCTYPE html>\n<html lang='en'>\n<head>\n<meta charset='UTF-8'>\n<title>Statistical Report</title>\n<link rel='stylesheet' href='../../resources/Styles/styles.css'>\n</head>\n<body>\n<div class='main'>\n<h1 id='statisticalReport'>Statistical Report - Workflow</h1>\n<h2 id='porechopABI'>Porechop ABI</h2>\n<h3>Trimming adapters from read ends</h3>\n"

# Creating the table containg everything
# The last 3 are summary lines from the adapters trimmed so can parse everything until then
#unorder_list = []
#unorder_list.append("<ul>\n")
#for adapter in adapters:
#
#    unorder_list.append(f"\t<li>{adapter}</li>\n")
#unorder_list.append("</ul>\n")

# Header adapter loc
adap_loc_header = "<h3>Adapters Removed</h3>\n"

# Adding the last line to the file
html_end = "</body>\n</html>"

# WRITING TO THE FILE
# The location of the outputfile
statisticalFile = args.outputFile

#Opening the outputfile to write to
with open(statisticalFile, "w") as html_file:
    # PORECHOP ABI
    html_file.writelines(html_header)
    # Adapters
    html_file.write("<table>\n\t<tr>\n\t\t<th>Adapter</th>\n\t\t<th>Sequence</th>\n\t</tr>\n")
    
    for adapter in adapters:
        splitted_adapter = adapter.split(":")
        html_file.writelines(f"\t<tr>\n\t\t<td>{splitted_adapter[0]}</td>\n\t\t<td>{splitted_adapter[1]}</td>\n\t</tr>\n")
    html_file.writelines("</table>\n")

    html_file.writelines(adap_loc_header)
    #Removed adapters
    html_file.writelines(f"\t<div class='centerdiv'>{StartAdapRem} / {StartTotReads} reads had adapters trimmed from their start ({StartBasePairs} bp removed)</div>")
    html_file.writelines(f"\t<div class='centerdiv'>{MidAdapRem} / {MidTotReads} reads had adapters trimmed from their middle </div>")
    html_file.writelines(f"\t<div class='centerdiv'>{EndAdapRem} / {EndTotReads} reads had adapters trimmed from their end ({EndBasePairs} bp removed)</div>")
    # PROWLER
    html_file.writelines("<h2 id='prowler'>Prowler Trimming</h2>\n")
    # Have to set the locatio from where the html file will be, so set picture in same folder
    html_file.writelines(f"\t<img src='{identifier}Before&After-Prowler.png' height='800px'>\n")

    # SACRA
    html_file.writelines("<h2 id='sacra'>Split Amplified Chimeric Read Algorithm (SACRA)</h2>\n")
    # Have to set the locatio from where the html file will be, so set picture in same folder
    html_file.writelines(f"\t<img src='{identifier}SACRA-Stacked-Seq-Amount.png' height='800px'>\n")
    html_file.writelines(f"\t<img src='{identifier}SACRA-Stacked-Seq-Rel-Amount.png' height='800px'>\n")
    html_file.writelines(f"\t<img src='{identifier}SACRA-Hist-Distribution.png' height='800px'>\n")
    # The statistical ouput after filtering on certain amount of bases
    html_file.writelines("<h2 id ='sacrafilt'>SACRA Filtered Reads</h2>\n")
    html_file.writelines(f"\t<img src='{identifier}SACRA-Hist-FilteredSeq.png' height='800px'>\n")

    ######################### DIAMOND #########################
    html_file.writelines("<h2 id='diamond'>DIAMOND</h2>\n")
    # Read the file containing the amount a hit has been found in the genes
    with open(f"../results/{identifier}/{identifier}HeaderCountDIAMOND.txt", "r") as text_reader:
        lines_read = text_reader.readlines()
        # Write the output in an unordered list
        html_file.writelines("<ul>\n")
        for line in lines_read:
            # Write each newline in a list item, strip the line for \n. 
            html_file.writelines(f"\t<li>{line.strip()}</li>\n")
        html_file.writelines("</ul>\n")
    # Writing the pictures to the html file. 
    html_file.writelines(f"\t<img src='{identifier}Bar-HitsPerGene-DIAMOND&Filtering.png' height='800px'>\n")
    html_file.writelines(f"\t<img src='{identifier}Hist-SequenceLengthAfterDIAMOND&Filtering.png' height='800px'>\n")
    #Add the end of html to the file. 
    html_file.writelines(html_end)

    ######################### FLYE - BANDAGE #########################
    html_file.writelines("<h2 id='bandage'>Flye - BANDAGE</h2>\n")
    html_file.writelines(f"\t<img src='{identifier}BANDAGE-FlyeAssembly.jpg' height='800px'>\n")

    ######################### MITOS2 ANNOTATION REPORT ###########################
    
    html_file.writelines("<h2 id='mitos'>MITOS2</h2>\n")

    # Filter the list to open files in numerical order. 
    list_folders = os.listdir(args.MITOS2Folder)
    list_folders.sort()
    # Empty contig list to create titles for navbar
    contig_headers = []
    # The IDS to link the headers to  
    ids_headers = []
    # Need to specify the full path to find the specific folder
    for folder in list_folders:
        #print(f"{folder}")
        # Have to give the full path with it to find the folder
        for file in os.listdir(f"{args.MITOS2Folder}/{folder}/"):
            ## GFF
            if re.search("result.gff",file):
                
                with open(f"{args.MITOS2Folder}/{folder}/{file}", "r") as file_to_read:
                    # Create a list of lines from the file
                    file_lines = file_to_read.readlines()
                    # The title by getting the contig name
                    splitted_item = file_lines[0].split("\t")
                    contig_raw = splitted_item[0]
                    # Edit word
                    contig_list = contig_raw.split("_")
                    # Format a title and save it into a variable
                    formatted = contig_list[0].capitalize() + " " + contig_list[1]
                    # Add the variable to the list 
                    contig_headers.append(formatted)
                    # Create string for the ids to link to
                    ids_headers.append(contig_list[1])
                    # Write title == Contig 
                    html_file.write(f"<h3 id='{contig_list[1]}'>{contig_list[0].capitalize()} {contig_list[1]}</h3>\n")
                    # Write the table header to the file
                    html_file.write("<table>\n\t<tr>\n\t\t<th>Source</th>\n\t\t<th>Feature</th>\n\t\t<th>Start position</th>\n\t\t<th>End position</th>\n\t\t<th>Score</th>\n\t\t<th>Strand</th>\n\t\t<th>Frame</th>\n\t\t<th>Attributes</th>\n\t</tr>\n")
                    # Iterate over the lines from the list
                    for line in file_lines:
                        # The columns are tab seperated o split on
                        splitted_gff = line.split("\t")
                        # 1) seqname - name of the chromosome or scaffold
                        # 2) source - name of the data source 
                        # 3) feature - feature type 
                        # 4) start - Start position
                        # 5) end - End position
                        # 6) score - A floating point value.
                        # 7) strand - defined as + (forward) or - (reverse).
                        # 8) frame - One of '0', '1' or '2'. '0' indicates that the first base of the feature is the first base of a codon, '1' that the second base is the first base of a codon, and so on..
                        # 9) attribute - A semicolon-separated list of tag-value pairs, providing additional information about each feature.
                        #print(splitted_gff)
                        # Parse everything to a table in the file. 
                        html_file.write(f"\t<tr>\n\t\t<td>{splitted_gff[1]}</td>\n\t\t<td>{splitted_gff[2]}</td>\n\t\t<td>{splitted_gff[3]}</td>\n\t\t<td>{splitted_gff[4]}</td>\n\t\t<td>{splitted_gff[5]}</td>\n\t\t<td>{splitted_gff[6]}</td>\n\t\t<td>{splitted_gff[7]}</td>\n\t\t<td>{splitted_gff[8].strip()}</td>\n\t</tr>\n")
                    html_file.write("</table>\n")
                file_to_read.close()

            ## FASTA   
            # Only open the file name that matches -- > Will be always the same
            if re.search("result.fas", file):
                # Open the file to read from
                with open(f"{args.MITOS2Folder}/{folder}/{file}", "r") as file_to_read:
                    # Get the lines of a file split on newline in a list. 
                    file_lines = file_to_read.readlines()
                    #Set a counter for each file to handle
                    count = 0
                    
                    for line in file_lines:
                        count += 1
                        # To write the first div
                        if re.search("^>", line) and count == 1:
                            html_file.write(f"\t<div class='contigpadd'>\n\t\t{line}\n\t<pre>\n")
                        # To write the divs except first and last
                        elif re.search("^>", line) and count != 1:
                            html_file.write(f"\t</pre>\n\t</div>\n\t<div class='contigpadd'>\n\t\t{line}\n\t<pre>\n")
                        # Write the sequence lines
                        elif re.search("^[A,G,C,T,U]", line):
                            # Tab in pre statement will directly visualized. 
                            html_file.write(f"\t{line}")
                    
                    # To write the last div block
                    if count == len(file_lines):
                        html_file.write(f"\t</pre>\n\t</div>\n")
                # Close file
                file_to_read.close()
    
    html_file.write(f"</div>\n")
    # Write a div to a file that will contain the part of the sidebar. 
    # Write every line connected to a title to the sidebar on the side. 
    html_file.write(f"<div class='sidenav'>\n")
    # Only style the first differently to come more out as a header. 
    html_file.write(f"<a class='header' href='#statisticalReport'>Statistical Report</a>\n")
    html_file.write(f"<a href='#porechopABI'>Porechop ABI</a>\n")
    html_file.write(f"<a href='#prowler'>Prowler Trimming</a>\n")
    html_file.write(f"<a href='#sacra'>SACRA</a>\n")
    html_file.write(f"<a href='#sacrafilt'>SACRA Filtered</a>\n")
    html_file.write(f"<a href='#diamond'>DIAMOND</a>\n")
    html_file.write("<a href='#bandage'>Flye - BANDAGE</a>\n")
    html_file.write(f"<a href='#mitos'>MITOS2</a>\n")
    # Loop over 2 lists one that contains the links to the titles & one with the visible title in the navbar
    for ids, item in zip(ids_headers, contig_headers):
        html_file.write(f"<a class='subtitle'href='#{ids}'>{item}</a>\n")
    # Close the div 
    html_file.write(f"</div>'>\n")

    #Close the file
    html_file.close()

# Close the file that has been written to
html_file.close()
# Does not work on the WSL ubuntu yet, could be because no browser installed on it
#time.sleep(2)
#webbrowser.open_new_tab("report.html")
